{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "from paperqa.agents.main import agent_query\n",
    "from paperqa.settings import Settings, AgentSettings, IndexSettings, AnswerSettings, ParsingSettings, PromptSettings\n",
    "\n",
    "from src.build_search_index import build_search_index, process_bibtex_and_pdfs, create_manifest_file\n",
    "from src.query_answer_index import query_answer_index\n",
    "from src.utils import pretty_print_text\n",
    "\n",
    "# configure the logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# Generate a timestamp for the log file name\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging_dir = Path(\".\") / \"logs\"\n",
    "logging_dir.mkdir(exist_ok=True)\n",
    "file_handler = logging.FileHandler(str(logging_dir / f\"log_{timestamp}.log\"))  # log file name\n",
    "file_handler.setLevel(logging.INFO)  # desired log level for the file\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(file_handler)\n",
    "\n",
    "# NOTE: these are the paths that should be configured\n",
    "export_directory_name = \"MS_EXPORT\"\n",
    "project_dir = Path(\".\")\n",
    "#project_dir = Path(\"/\") / \"Users\" / \"pschafer\" / \"Projects\" / \"MS_expert\"\n",
    "\n",
    "# default paths\n",
    "data_dir = project_dir / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "paper_directory = data_dir / export_directory_name\n",
    "index_directory = data_dir / f\"{export_directory_name}_index\"\n",
    "bibtex_file = paper_directory / f\"{export_directory_name}.bib\"\n",
    "manifest_file = data_dir / f\"{export_directory_name}_manifest.csv\"\n",
    "index_name = f\"pqa_index_{export_directory_name}\"\n",
    "\n",
    "# create manifest file from bibtex\n",
    "processed_df = process_bibtex_and_pdfs(bibtex_file=bibtex_file, paper_directory=paper_directory)\n",
    "create_manifest_file(manifest_df=processed_df, manifest_file=manifest_file)\n",
    "\n",
    "#manifest_from_bibtex(bibtex_file=bibtex_file, \n",
    "#                     paper_directory=paper_directory, \n",
    "#                     manifest_file=manifest_file)\n",
    "\n",
    "# set paperQA settings\n",
    "index_settings = IndexSettings(\n",
    "    name = index_name,\n",
    "    paper_directory = paper_directory,\n",
    "    manifest_file = manifest_file,\n",
    "    index_directory = index_directory,\n",
    "    use_absolute_paper_directory = False,\n",
    "    recurse_subdirectories = True,\n",
    "    concurrency = 1, # \"number of concurrent filesystem reads for indexing (probably not important anymore since I avoid calling S2)\"\n",
    ")\n",
    "\n",
    "agent_settings = AgentSettings(\n",
    "    agent_llm = \"gpt-4o-mini\", # smaller than default (bc cheaper)\n",
    "    index = index_settings,\n",
    "    index_concurrency = index_settings.concurrency\n",
    ")\n",
    "\n",
    "answer_settings = AnswerSettings(\n",
    "    evidence_k = 10, # number of evidence text chunks to retrieve (default=10)\n",
    "    evidence_summary_length = \"about 100 words\", # length of evidence summary (default=\"about 100 words\")\n",
    "    answer_max_sources = 5, # max number of sources to use for answering (default=5)\n",
    "    answer_length = \"about 200 words, but can be longer\", # length of final answer (default=\"about 200 words, but can be longer\")\n",
    ")\n",
    "\n",
    "parse_settings = ParsingSettings()\n",
    "\n",
    "prompt_settings = PromptSettings()\n",
    "\n",
    "settings = Settings(\n",
    "    agent = agent_settings, \n",
    "    answer = answer_settings,\n",
    "    parsing = parse_settings,\n",
    "    prompts = prompt_settings,\n",
    "    llm=\"gpt-4o-mini\", # smaller than default (bc cheaper)\n",
    "    summary_llm=\"gpt-4o-mini\", # smaller than default (bc cheaper)\n",
    "    embedding=\"text-embedding-3-small\", # default\n",
    "    temperature = 0.0, # default\n",
    "    texts_index_mmr_lambda = 1.0, # Lambda MMR (default)\n",
    "    index_absolute_directory = index_settings.use_absolute_paper_directory,\n",
    "    index_directory = index_settings.index_directory,\n",
    "    index_recursively = index_settings.recurse_subdirectories,\n",
    "    manifest_file = index_settings.manifest_file,\n",
    "    paper_directory = index_settings.paper_directory,\n",
    "    verbosity = 0, # (0-3), where 3 is all LLM/Embedding calls are logged\n",
    ")\n",
    "\n",
    "# Make sure that I am using the defautl arguments where it matters\n",
    "def print_non_default_settings(settings_defined, settings_classs, settings_name):\n",
    "    print(f\"------\\n{settings_name}\")\n",
    "    for key, value in settings_defined.__dict__.items():\n",
    "        default_value = getattr(settings_classs(), key, None)\n",
    "        if value != default_value:\n",
    "            print(f\"selected: {key}: {value}\")\n",
    "            print(f\"-> default: {key}: {default_value}\")\n",
    "\n",
    "# Print non-default settings for each object\n",
    "#print_non_default_settings(index_settings, IndexSettings, \"index_settings\")\n",
    "#print_non_default_settings(agent_settings, AgentSettings, \"agent_settings\")\n",
    "#print_non_default_settings(answer_settings, AnswerSettings, \"answer_settings\")\n",
    "#print_non_default_settings(parse_settings, ParsingSettings, \"parse_settings\")\n",
    "#print_non_default_settings(prompt_settings, PromptSettings, \"prompt_settings\")\n",
    "#print_non_default_settings(settings, Settings, \"settings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index = await build_search_index(settings=settings, bibtex_file=bibtex_file, manifest_file=manifest_file)\n",
    "assert search_index.index_name == settings.agent.index.name\n",
    "print(f\"Index Name: {search_index.index_name}\")\n",
    "print(f\"Number of Indexed Files: {len((await search_index.index_files).keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import anyio\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bibtexparser\n",
    "import pymupdf\n",
    "\n",
    "from paperqa.settings import Settings\n",
    "from paperqa.agents.search import SearchIndex\n",
    "from paperqa.utils import maybe_is_text, md5sum\n",
    "from paperqa.docs import Docs, Doc, DocDetails, read_doc        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_df = pd.read_csv(manifest_file)\n",
    "manifest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers\" in t for t in manifest_df.title.to_list()])\n",
    "subset = manifest_df.loc[[\"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers\" in t for t in manifest_df.title.to_list()], :]\n",
    "subset.file_location.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bibtex_file, \"r\") as bibfile:\n",
    "    bib_database = bibtexparser.load(bibfile)\n",
    "bib_list = bib_database.entries\n",
    "# NOTE: We only use PDF for now!\n",
    "bib_dict = {re.search(r'[^;]*\\.pdf', entry[\"file\"]).group(0): entry for entry in bib_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import anyio\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bibtexparser\n",
    "import pymupdf\n",
    "\n",
    "from paperqa.settings import Settings\n",
    "from paperqa.agents.search import SearchIndex\n",
    "from paperqa.utils import maybe_is_text, md5sum\n",
    "from paperqa.docs import Docs, Doc, DocDetails, read_doc\n",
    "\n",
    "def _format_authors(bibtex_authors):\n",
    "    # Split authors by \"and\"\n",
    "    authors = [author.strip() for author in bibtex_authors.split(\" and \")]\n",
    "    \n",
    "    # Reformat each author from \"Last, First\" to \"First Last\"\n",
    "    formatted_authors = []\n",
    "    for author in authors:\n",
    "        if ',' in author:\n",
    "            last, first = map(str.strip, author.split(',', 1))\n",
    "            formatted_authors.append(f\"{first} {last}\")\n",
    "        else:\n",
    "            formatted_authors.append(author)  # In case no comma, keep as is\n",
    "    \n",
    "    return formatted_authors\n",
    "\n",
    "rel_file_path = \"files/27191/Tortosa-Carreres et al. - 2024 - Predictive potential of serum and cerebrospinal fluid biomarkers for disease activity in treated mul.pdf\"\n",
    "\n",
    "docs_for_single_doc = Docs() # otherwise creating Docs object for single doc confuses me\n",
    "\n",
    "abs_file_path = settings.paper_directory / rel_file_path\n",
    "\n",
    "bib_dict_doc = bib_dict[str(rel_file_path)]\n",
    "if \"author\" in bib_dict_doc.keys():\n",
    "    bib_dict_doc[\"authors\"] = _format_authors(bib_dict_doc[\"author\"])\n",
    "\n",
    "parse_config = settings.parsing\n",
    "dockey = md5sum(abs_file_path)\n",
    "\n",
    "llm_model = settings.get_llm()\n",
    "\n",
    "texts = read_doc(\n",
    "        abs_file_path,\n",
    "        Doc(docname=\"\", citation=\"\", dockey=dockey),  # Fake doc\n",
    "        chunk_chars=parse_config.chunk_size,\n",
    "        overlap=parse_config.overlap,\n",
    "        page_size_limit=parse_config.page_size_limit,\n",
    "    )\n",
    "\n",
    "if not texts:\n",
    "    raise ValueError(f\"Could not read document {abs_file_path}. Is it empty?\")\n",
    "\n",
    "result = await llm_model.run_prompt(\n",
    "    prompt=parse_config.citation_prompt,\n",
    "    data={\"text\": texts[0].text},\n",
    "    system_prompt=None,  # skip system because it's too hesitant to answer\n",
    ")\n",
    "citation = result.text\n",
    "\n",
    "if (\n",
    "    len(citation) < 3  # noqa: PLR2004\n",
    "    or \"Unknown\" in citation\n",
    "    or \"insufficient\" in citation\n",
    "):\n",
    "    citation = f\"Unknown, {os.path.basename(abs_file_path)}, {datetime.now().year}\"\n",
    "\n",
    "# get first name and year from citation\n",
    "match = re.search(r\"([A-Z][a-z]+)\", citation)\n",
    "if match is not None:\n",
    "    author = match.group(1)\n",
    "else:\n",
    "    # panicking - no word??\n",
    "    raise ValueError(\n",
    "        f\"Could not parse docname from citation {citation}. \"\n",
    "        \"Consider just passing key explicitly - e.g. docs.py \"\n",
    "        \"(path, citation, key='mykey')\"\n",
    "    )\n",
    "year = \"\"\n",
    "match = re.search(r\"(\\d{4})\", citation)\n",
    "if match is not None:\n",
    "    year = match.group(1)\n",
    "docname = f\"{author}{year}\"\n",
    "docname = docs_for_single_doc._get_unique_name(docname)\n",
    "\n",
    "doc = Doc(docname=docname, citation=citation, dockey=dockey)\n",
    "\n",
    "# see also CROSSREF_API_MAPPING, SEMANTIC_SCHOLAR_API_MAPPING\n",
    "\n",
    "doc_details = DocDetails(**{k: bib_dict_doc[k] for k in [\"doi\", \"authors\", \"title\", \"year\", \"publisher\", \"issn\", \"volume\", \"pages\", \"journal\"] if k in bib_dict_doc.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIBTEX_ATTR = [\"doi\", \"authors\", \"title\", \"year\", \"publisher\", \"issn\", \"volume\", \"pages\", \"journal\"]\n",
    "doc_details_dict = {k: bib_dict_doc[k] for k in BIBTEX_ATTR if k in bib_dict_doc.keys()}\n",
    "doc_details_dict\n",
    "doc_details_dict[\"authors\"] = [auth.replace(\"{\", \"\").replace(\"}\", \"\") for auth in doc_details_dict[\"authors\"]]\n",
    "#del doc_details_dict[\"authors\"]\n",
    "DocDetails(**doc_details_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_details_dict[\"authors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a.replace(\"{\", \"\").replace(\"}\", \"\") for a in doc_details_dict[\"authors\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DocDetails class\n",
    "doc_details = DocDetails(\n",
    "    title=\"My Awesome Document\",\n",
    "    authors=[\"John Doe\", \"Jane Doe\"],\n",
    "    year=2023,\n",
    "    # ... other attributes\n",
    ")\n",
    "doc_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \n",
    "data = DocDetails.lowercase_doi_and_populate_doc_id(data)\n",
    "data = DocDetails.remove_invalid_authors(data)\n",
    "data = DocDetails.misc_string_cleaning(data)\n",
    "data = DocDetails.inject_clean_doi_url_into_data(data)\n",
    "data = DocDetails.add_preprint_journal_from_doi_if_missing(data)\n",
    "data = DocDetails.populate_bibtex_key_citation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocDetails.populate_bibtex_key_citation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_dict_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_response = await agent_query(\n",
    "    query=\"What do you know about the perivascular niche/space in multiple sclerosis\", \n",
    "    settings=settings, rebuild_index=False\n",
    ")\n",
    "pretty_print_text(answer_response.session.formatted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Previous Question & Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_answer_index_results = await query_answer_index(settings=settings, query=\"role of perivascular niche in MS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperqa.utils import parse_string, clean_upbibtex, unsrtalpha, CitationConversionError, Person, Parser, FieldIsMissing\n",
    "\n",
    "\n",
    "def format_bibtex(\n",
    "    bibtex: str,\n",
    "    key: str | None = None,\n",
    "    clean: bool = True,\n",
    "    missing_replacements: dict[str, str] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Transform bibtex entry into a citation, potentially adding missing fields.\"\"\"\n",
    "    style = unsrtalpha.Style()\n",
    "    if missing_replacements is None:\n",
    "        missing_replacements = {}\n",
    "    try:\n",
    "        bd = parse_string(clean_upbibtex(bibtex) if clean else bibtex, \"bibtex\")\n",
    "        key = list(bd.entries.keys())[0]\n",
    "    except Exception:\n",
    "        key = bibtex.split(\",\")[0]\n",
    "        key = key.replace(\"{{\", \"{\")\n",
    "        key = key.split(\"{\")[1]\n",
    "        return \"Ref \" + key\n",
    "    try:\n",
    "        entry = bd.entries[key]\n",
    "    except KeyError as exc:  # Let's check if key is a non-empty prefix\n",
    "        try:\n",
    "            entry = next(\n",
    "                iter(v for k, v in bd.entries.items() if k.startswith(key) and key)\n",
    "            )\n",
    "        except StopIteration:\n",
    "            raise CitationConversionError(\n",
    "                f\"Failed to process{' and clean up' if clean else ''} bibtex {bibtex}\"\n",
    "                f\" due to failed lookup of key {key}.\"\n",
    "            ) from exc\n",
    "    try:\n",
    "        # see if we can insert missing fields\n",
    "        for field, replacement_value in missing_replacements.items():\n",
    "            # Deal with special case for author, since it needs to be parsed\n",
    "            # into Person objects. This reorganizes the names automatically.\n",
    "            if field == \"author\" and \"author\" not in entry.persons:\n",
    "                tmp_author_bibtex = f\"@misc{{tmpkey, author={{{replacement_value}}}}}\"\n",
    "                authors: list[Person] = (\n",
    "                    Parser()\n",
    "                    .parse_string(tmp_author_bibtex)\n",
    "                    .entries[\"tmpkey\"]\n",
    "                    .persons[\"author\"]\n",
    "                )\n",
    "                for a in authors:\n",
    "                    entry.add_person(a, \"author\")\n",
    "            elif field not in entry.fields:\n",
    "                entry.fields.update({field: replacement_value})\n",
    "        entry = style.format_entry(label=\"1\", entry=entry)\n",
    "        return entry.text.render_as(\"text\")\n",
    "    except (FieldIsMissing, UnicodeDecodeError):\n",
    "        try:\n",
    "            return entry.fields[\"title\"]\n",
    "        except KeyError as exc:\n",
    "            raise CitationConversionError(\n",
    "                f\"Failed to process{' and clean up' if clean else ''} bibtex {bibtex}\"\n",
    "                \" due to missing a 'title' field.\"\n",
    "            ) from exc\n",
    "    \n",
    "missing_replacements = None\n",
    "clean = True\n",
    "bibtex = \"\"\"@article{{tortosacarreres}2024predictivepotentialof,\n",
    "    author = \"{Tortosa-Carreres}, Jordi and {Cubas-N{\\'u}{\\\\textasciitilde n}ez}, Laura and {Quiroga-Varela}, Ana and {Castillo-Villalba}, Jessica and {Rami{\\'o}-Torrenta}, Llu{\\'i}s and Piqueras, M{\\'o}nica and {Gasqu{\\'e}-Rubio}, Raquel and {Quintanilla-Bordas}, Carlos and Sanz, Maria Teresa and Lucas, Celia and {Huertas-Pons}, Joana Mar{\\'i}a and Miguela, Albert and Casanova, Bonaventura and {Laiz-Marro}, Bego{\\\\textasciitilde n}a and {P{\\'e}rez-Miralles}, Francisco Carlos\",\n",
    "    title = \"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers for Disease Activity in Treated Multiple Sclerosis Patients\",\n",
    "    year = \"2024\",\n",
    "    journal = \"Multiple Sclerosis and Related Disorders\",\n",
    "    doi = \"10.1016/j.msard.2024.105734\",\n",
    "    url = \"https://doi.org/10.1016/j.msard.2024.105734\",\n",
    "    publisher = \"Elsevier\",\n",
    "    issn = \"2211-0348, 2211-0356\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Transform bibtex entry into a citation, potentially adding missing fields.\"\"\"\n",
    "style = unsrtalpha.Style()\n",
    "if missing_replacements is None:\n",
    "    missing_replacements = {}\n",
    "try:\n",
    "    print(bibtex)\n",
    "    bd = parse_string(clean_upbibtex(bibtex) if clean else bibtex, \"bibtex\")\n",
    "    key = list(bd.entries.keys())[0]\n",
    "except Exception:\n",
    "    key = bibtex.split(\",\")[0]\n",
    "    key = key.replace(\"{{\", \"{\")\n",
    "    key = key.split(\"{\")[1]\n",
    "    print(\"Ref \" + key)\n",
    "try:\n",
    "    entry = bd.entries[key]\n",
    "    print(entry)\n",
    "except KeyError as exc:  # Let's check if key is a non-empty prefix\n",
    "    try:\n",
    "        entry = next(\n",
    "            iter(v for k, v in bd.entries.items() if k.startswith(key) and key)\n",
    "        )\n",
    "    except StopIteration:\n",
    "        raise CitationConversionError(\n",
    "            f\"Failed to process{' and clean up' if clean else ''} bibtex {bibtex}\"\n",
    "            f\" due to failed lookup of key {key}.\"\n",
    "        ) from exc\n",
    "try:\n",
    "    # see if we can insert missing fields\n",
    "    for field, replacement_value in missing_replacements.items():\n",
    "        # Deal with special case for author, since it needs to be parsed\n",
    "        # into Person objects. This reorganizes the names automatically.\n",
    "        if field == \"author\" and \"author\" not in entry.persons:\n",
    "            tmp_author_bibtex = f\"@misc{{tmpkey, author={{{replacement_value}}}}}\"\n",
    "            authors: list[Person] = (\n",
    "                Parser()\n",
    "                .parse_string(tmp_author_bibtex)\n",
    "                .entries[\"tmpkey\"]\n",
    "                .persons[\"author\"]\n",
    "            )\n",
    "            for a in authors:\n",
    "                entry.add_person(a, \"author\")\n",
    "        elif field not in entry.fields:\n",
    "            entry.fields.update({field: replacement_value})\n",
    "    entry = style.format_entry(label=\"1\", entry=entry)\n",
    "    print(entry.text.render_as(\"text\"))\n",
    "except (FieldIsMissing, UnicodeDecodeError):\n",
    "    try:\n",
    "        print(entry.fields[\"title\"])\n",
    "    except KeyError as exc:\n",
    "        raise CitationConversionError(\n",
    "            f\"Failed to process{' and clean up' if clean else ''} bibtex {bibtex}\"\n",
    "            \" due to missing a 'title' field.\"\n",
    "        ) from exc\n",
    "\n",
    "\n",
    "#test = \"\"\"@article{{tortosacarreres}2024predictivepotentialof,\n",
    "#    author = \"{Tortosa-Carreres}, Jordi and {Cubas-N{\\'u}{\\\\textasciitilde n}ez}, Laura and {Quiroga-Varela}, Ana and {Castillo-Villalba}, Jessica and {Rami{\\'o}-Torrenta}, Llu{\\'i}s and Piqueras, M{\\'o}nica and {Gasqu{\\'e}-Rubio}, Raquel and {Quintanilla-Bordas}, Carlos and Sanz, Maria Teresa and Lucas, Celia and {Huertas-Pons}, Joana Mar{\\'i}a and Miguela, Albert and Casanova, Bonaventura and {Laiz-Marro}, Bego{\\\\textasciitilde n}a and {P{\\'e}rez-Miralles}, Francisco Carlos\",\n",
    "#    title = \"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers for Disease Activity in Treated Multiple Sclerosis Patients\",\n",
    "#    year = \"2024\",\n",
    "#    journal = \"Multiple Sclerosis and Related Disorders\",\n",
    "#    doi = \"10.1016/j.msard.2024.105734\",\n",
    "#    url = \"https://doi.org/10.1016/j.msard.2024.105734\",\n",
    "#    publisher = \"Elsevier\",\n",
    "#    issn = \"2211-0348, 2211-0356\"\n",
    "#\"\"\"\n",
    "#format_bibtex(test)\n",
    "#parse_string(clean_upbibtex(test), \"bibtex\")\n",
    "\n",
    "bd = parse_string(clean_upbibtex(bibtex) if clean else bibtex, \"bibtex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bibtex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"@article{tortosacarreres2024predictivepotentialof,\n",
    "  author = \"Tortosa-Carreres, Jordi and Cubas-Núñez, Laura and Quiroga-Varela, Ana and Castillo-Villalba, Jessica and Ramió-Torrenta, Lluís and Piqueras, Mónica and Gasqué-Rubio, Raquel and Quintanilla-Bordas, Carlos and Sanz, Maria Teresa and Lucas, Celia and Huertas-Pons, Joana María and Miguela, Albert and Casanova, Bonaventura and Laiz-Marro, Begoña and Pérez-Miralles, Francisco Carlos\",\n",
    "  title = \"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers for Disease Activity in Treated Multiple Sclerosis Patients\",\n",
    "  year = \"2024\",\n",
    "  journal = \"Multiple Sclerosis and Related Disorders\",\n",
    "  doi = \"10.1016/j.msard.2024.105734\",\n",
    "  url = \"https://doi.org/10.1016/j.msard.2024.105734\",\n",
    "  publisher = \"Elsevier\",\n",
    "  issn = \"2211-0348, 2211-0356\"\n",
    "}\"\"\"\n",
    "parse_string(test, bib_format=\"bibtex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"@article{tortosacarreres2024predictivepotentialof,\n",
    "    author = \"foo, bar\",\n",
    "    title = \"Predictive Potential of Serum and Cerebrospinal Fluid Biomarkers for Disease Activity in Treated Multiple Sclerosis Patients\",\n",
    "    year = \"2024\",\n",
    "    journal = \"Multiple Sclerosis and Related Disorders\",\n",
    "    doi = \"10.1016/j.msard.2024.105734\",\n",
    "    url = \"https://doi.org/10.1016/j.msard.2024.105734\",\n",
    "    publisher = \"Elsevier\",\n",
    "    issn = \"2211-0348\"\n",
    "}\"\"\"\n",
    "parse_string(test, bib_format=\"bibtex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocDetails(**doc_details_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybtex\n",
    "\n",
    "bib_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = False\n",
    "bd = parse_string(clean_upbibtex(test) if clean else test, \"bibtex\")\n",
    "list(bd.entries.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = parse_string(clean_upbibtex(bibtex) if clean else bibtex, \"bibtex\")\n",
    "bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperqa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
